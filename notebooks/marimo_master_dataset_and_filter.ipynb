{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "Hbol",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MJUe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import gzip\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import marimo as mo\n",
    "\n",
    "import gdown\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from Bio import SeqIO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vblA",
   "metadata": {},
   "source": [
    "# Data Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bkHC",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSource:\n",
    "    \"\"\"Base class for data sources.\"\"\"\n",
    "\n",
    "    def __init__(self, data, filepath):\n",
    "        self.raw_data = data\n",
    "        self.filepath = filepath\n",
    "\n",
    "    @property\n",
    "    def data(self):\n",
    "        return self.raw_data\n",
    "\n",
    "\n",
    "class ReferenceGenome(DataSource):\n",
    "    \"\"\"Object for quickly loading and querying the reference genome.\"\"\"\n",
    "\n",
    "    @classmethod\n",
    "    def from_path(cls, path):\n",
    "        genome_dict = {record.id: str(record.seq).upper() for record in SeqIO.parse(path, \"fasta\")}\n",
    "        return cls(genome_dict, path)\n",
    "\n",
    "    @classmethod\n",
    "    def from_dict(cls, data_dict):\n",
    "        return cls(data_dict, filepath=None)\n",
    "\n",
    "    @property\n",
    "    def genome(self):\n",
    "        return self.data\n",
    "\n",
    "    def sequence(self, chrom, start, end):\n",
    "        chrom_sequence = self.genome[chrom]\n",
    "\n",
    "        assert end < len(chrom_sequence), (\n",
    "            f\"Sequence position bound out of range for chromosome {chrom}. \"\n",
    "            f\"{chrom} length {len(chrom_sequence)}, requested position {end}.\"\n",
    "        )\n",
    "        return chrom_sequence[start:end]\n",
    "\n",
    "\n",
    "class FilteringData:\n",
    "    \"\"\"Class for filtering exclusive peaks between replicates.\"\"\"\n",
    "\n",
    "    def __init__(self, df: pd.DataFrame, cell_list: list):\n",
    "        self.df = df\n",
    "        self.cell_list = cell_list\n",
    "        self._test_data_structure()\n",
    "\n",
    "    def _test_data_structure(self):\n",
    "        # Ensures all columns after the 11th are named cell names\n",
    "        assert all(\"_ENCL\" in x for x in self.df.columns[11:]), \"_ENCL not in all columns after 11th\"\n",
    "\n",
    "    def filter_exclusive_replicates(self, sort: bool = False, balance: bool = True):\n",
    "        \"\"\"Given a specific set of samples (one per cell type),\n",
    "        capture the exclusive peaks of each samples (the ones matching just one sample for the whole set)\n",
    "        and then filter the dataset to keep only these peaks.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: The original dataframe plus a column for each cell type with the exclusive peaks\n",
    "        \"\"\"\n",
    "        print(\"Filtering exclusive peaks between replicates\")\n",
    "        # Selecting the columns corresponding to the cell types\n",
    "        subset_cols = self.df.columns[:11].tolist() + self.cell_list\n",
    "        # Creating a new dataframe with only the columns corresponding to the cell types\n",
    "        df_subset = self.df[subset_cols].copy()\n",
    "        # Creating a new column for each cell type with the exclusive peaks or 'NO_TAG' if not exclusive\n",
    "        df_subset[\"TAG\"] = df_subset[self.cell_list].apply(lambda x: \"NO_TAG\" if x.sum() != 1 else x.idxmax(), axis=1)\n",
    "\n",
    "        # Creating a new dataframe with only the rows with exclusive peaks\n",
    "        new_df_list = []\n",
    "        for k, v in df_subset.groupby(\"TAG\"):\n",
    "            v = v.copy()\n",
    "            if k != \"NO_TAG\":\n",
    "                cell, replicate = \"_\".join(k.split(\"_\")[:-1]), k.split(\"_\")[-1]\n",
    "                v[\"additional_replicates_with_peak\"] = (\n",
    "                    self.df[self.df.filter(like=cell).columns].apply(lambda x: x.sum(), axis=1).loc[v.index] - 1\n",
    "                )\n",
    "                print(f\"Cell type: {cell}, Replicate: {replicate}, Number of exclusive peaks: {v.shape[0]}\")\n",
    "            else:\n",
    "                v[\"additional_replicates_with_peak\"] = 0\n",
    "            new_df_list.append(v)\n",
    "        new_df = pd.concat(new_df_list).sort_index()\n",
    "        new_df[\"other_samples_with_peak_not_considering_reps\"] = (\n",
    "            new_df[\"numsamples\"] - new_df[\"additional_replicates_with_peak\"] - 1\n",
    "        )\n",
    "\n",
    "        # Sorting the dataframe by the number of samples with the peak\n",
    "        if sort:\n",
    "            new_df = pd.concat(\n",
    "                [\n",
    "                    x_v.sort_values(\n",
    "                        by=[\"additional_replicates_with_peak\", \"other_samples_with_peak_not_considering_reps\"],\n",
    "                        ascending=[False, True],\n",
    "                    )\n",
    "                    for x_k, x_v in new_df.groupby(\"TAG\")\n",
    "                ],\n",
    "                ignore_index=True,\n",
    "            )\n",
    "\n",
    "        # Balancing the dataset\n",
    "        if balance:\n",
    "            lowest_peak_count = new_df.groupby(\"TAG\").count()[\"sequence\"].min()\n",
    "            new_df = pd.concat(\n",
    "                [v_bal.head(lowest_peak_count) for k_bal, v_bal in new_df.groupby(\"TAG\") if k_bal != \"NO_TAG\"]\n",
    "            )\n",
    "\n",
    "        return new_df\n",
    "\n",
    "\n",
    "def download_file(url: str, filename: str, force_download: bool = False):\n",
    "    \"\"\"Download a file from a URL if it doesn't exist.\"\"\"\n",
    "    if os.path.exists(filename) and not force_download:\n",
    "        print(f\"File {filename} already exists, skipping download\")\n",
    "        return\n",
    "\n",
    "    print(f\"Downloading {filename}...\")\n",
    "    response = requests.get(url, stream=True)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    with open(filename, \"wb\") as f:\n",
    "        for chunk in response.iter_content(chunk_size=8192):\n",
    "            f.write(chunk)\n",
    "    print(f\"Downloaded {filename}\")\n",
    "\n",
    "\n",
    "def decompress_gz_file(gz_filename: str, force_decompress: bool = False):\n",
    "    \"\"\"Decompress a gzip file.\"\"\"\n",
    "    output_filename = gz_filename[:-3]  # Remove .gz extension\n",
    "\n",
    "    if os.path.exists(output_filename) and not force_decompress:\n",
    "        print(f\"File {output_filename} already exists, skipping decompression\")\n",
    "        return output_filename\n",
    "\n",
    "    print(f\"Decompressing {gz_filename}...\")\n",
    "    with gzip.open(gz_filename, \"rb\") as f_in:\n",
    "        with open(output_filename, \"wb\") as f_out:\n",
    "            shutil.copyfileobj(f_in, f_out)\n",
    "    print(f\"Decompressed to {output_filename}\")\n",
    "    return output_filename\n",
    "\n",
    "\n",
    "def sequence_bounds(summit: int, start: int, end: int, length: int):\n",
    "    \"\"\"Calculate the sequence coordinates (bounds) for a given DHS.\"\"\"\n",
    "    half = length // 2\n",
    "\n",
    "    if (summit - start) < half:\n",
    "        return start, start + length\n",
    "    elif (end - summit) < half:\n",
    "        return end - length, end\n",
    "\n",
    "    return summit - half, summit + half\n",
    "\n",
    "\n",
    "def add_sequence_column(df: pd.DataFrame, genome, length: int):\n",
    "    \"\"\"\n",
    "    Query the reference genome for each DHS and add the raw sequences\n",
    "    to the dataframe.\n",
    "    \"\"\"\n",
    "    seqs = []\n",
    "    for _, row in df.iterrows():\n",
    "        l, r = sequence_bounds(row[\"summit\"], row[\"start\"], row[\"end\"], length)\n",
    "        seq = genome.sequence(row[\"seqname\"], l, r)\n",
    "        seqs.append(seq)\n",
    "\n",
    "    df[\"sequence\"] = seqs\n",
    "    return df\n",
    "\n",
    "\n",
    "def create_master_dataset(data_dir: Path = Path(\"./data\"), force_download: bool = False):\n",
    "    \"\"\"Create the master dataset by downloading and processing all required files.\"\"\"\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "    # Define component columns\n",
    "    COMPONENT_COLUMNS = [f\"C{i}\" for i in range(1, 17)]\n",
    "\n",
    "    # Step 1: Download and process genome\n",
    "    print(\"\\n=== Step 1: Downloading and processing genome ===\")\n",
    "    genome_gz = data_dir / \"hg38.fa.gz\"\n",
    "    genome_path = data_dir / \"hg38.fa\"\n",
    "\n",
    "    if not genome_path.exists() or force_download:\n",
    "        download_file(\n",
    "            \"https://hgdownload.soe.ucsc.edu/goldenPath/hg38/bigZips/hg38.fa.gz\", str(genome_gz), force_download\n",
    "        )\n",
    "        decompress_gz_file(str(genome_gz), force_download)\n",
    "\n",
    "    print(\"Loading genome...\")\n",
    "    genome = ReferenceGenome.from_path(str(genome_path))\n",
    "\n",
    "    # Step 2: Download and process metadata\n",
    "    print(\"\\n=== Step 2: Downloading and processing metadata ===\")\n",
    "    metadata_file = data_dir / \"DHS_Index_and_Vocabulary_metadata.tsv\"\n",
    "    download_file(\n",
    "        \"https://www.meuleman.org/DHS_Index_and_Vocabulary_metadata.tsv\", str(metadata_file), force_download\n",
    "    )\n",
    "\n",
    "    DHS_metadata = pd.read_table(str(metadata_file)).iloc[:-1]  # Last row is empty\n",
    "\n",
    "    # Step 3: Download and process basis array\n",
    "    print(\"\\n=== Step 3: Downloading and processing basis array ===\")\n",
    "    basis_gz = data_dir / \"2018-06-08NC16_NNDSVD_Basis.npy.gz\"\n",
    "    basis_npy = data_dir / \"2018-06-08NC16_NNDSVD_Basis.npy\"\n",
    "\n",
    "    if not basis_npy.exists() or force_download:\n",
    "        download_file(\n",
    "            \"https://zenodo.org/record/3838751/files/2018-06-08NC16_NNDSVD_Basis.npy.gz?download=1\",\n",
    "            str(basis_gz),\n",
    "            force_download,\n",
    "        )\n",
    "        decompress_gz_file(str(basis_gz), force_download)\n",
    "\n",
    "    basis_array = np.load(str(basis_npy))\n",
    "    nmf_loadings = pd.DataFrame(basis_array, columns=COMPONENT_COLUMNS)\n",
    "    DHS_metadata = pd.concat([DHS_metadata, nmf_loadings], axis=1)\n",
    "    DHS_metadata[\"component\"] = (\n",
    "        DHS_metadata[COMPONENT_COLUMNS].idxmax(axis=1).apply(lambda x: int(x[1:]))\n",
    "    )\n",
    "\n",
    "    # Step 4: Download and process mixture array\n",
    "    print(\"\\n=== Step 4: Downloading and processing mixture array (this may take 10+ minutes) ===\")\n",
    "    mixture_gz = data_dir / \"2018-06-08NC16_NNDSVD_Mixture.npy.gz\"\n",
    "    mixture_npy = data_dir / \"2018-06-08NC16_NNDSVD_Mixture.npy\"\n",
    "\n",
    "    if not mixture_npy.exists() or force_download:\n",
    "        download_file(\n",
    "            \"https://zenodo.org/record/3838751/files/2018-06-08NC16_NNDSVD_Mixture.npy.gz?download=1\",\n",
    "            str(mixture_gz),\n",
    "            force_download,\n",
    "        )\n",
    "        decompress_gz_file(str(mixture_gz), force_download)\n",
    "\n",
    "    print(\"Loading mixture array...\")\n",
    "    mixture_array = np.load(str(mixture_npy)).T\n",
    "    nmf_loadings = pd.DataFrame(mixture_array, columns=COMPONENT_COLUMNS)\n",
    "\n",
    "    # Step 5: Download and process sequence metadata\n",
    "    print(\"\\n=== Step 5: Downloading and processing sequence metadata ===\")\n",
    "    seq_metadata_gz = data_dir / \"DHS_Index_and_Vocabulary_hg38_WM20190703.txt.gz\"\n",
    "    seq_metadata_file = data_dir / \"DHS_Index_and_Vocabulary_hg38_WM20190703.txt\"\n",
    "\n",
    "    if not seq_metadata_file.exists() or force_download:\n",
    "        download_file(\n",
    "            \"https://www.meuleman.org/DHS_Index_and_Vocabulary_hg38_WM20190703.txt.gz\",\n",
    "            str(seq_metadata_gz),\n",
    "            force_download,\n",
    "        )\n",
    "        decompress_gz_file(str(seq_metadata_gz), force_download)\n",
    "\n",
    "    print(\"Loading sequence metadata...\")\n",
    "    sequence_metadata = pd.read_table(str(seq_metadata_file), sep=\"\\t\")\n",
    "    sequence_metadata = sequence_metadata.drop(columns=[\"component\"], axis=1)\n",
    "\n",
    "    # Join metadata with component presence matrix\n",
    "    df = pd.concat([sequence_metadata, nmf_loadings], axis=1, sort=False)\n",
    "\n",
    "    # Add additional columns\n",
    "    df[\"component\"] = df[COMPONENT_COLUMNS].idxmax(axis=1).apply(lambda x: int(x[1:]))\n",
    "    df[\"proportion\"] = df[COMPONENT_COLUMNS].max(axis=1) / df[COMPONENT_COLUMNS].sum(axis=1)\n",
    "    df[\"total_signal\"] = df[\"mean_signal\"] * df[\"numsamples\"]\n",
    "    df[\"dhs_id\"] = df[[\"seqname\", \"start\", \"end\", \"summit\"]].apply(lambda x: \"_\".join(map(str, x)), axis=1)\n",
    "    df[\"DHS_width\"] = df[\"end\"] - df[\"start\"]\n",
    "\n",
    "    # Add sequences\n",
    "    print(\"Adding sequences from genome...\")\n",
    "    df = add_sequence_column(df, genome, 200)\n",
    "\n",
    "    # Rename and reorder columns\n",
    "    df = df.rename(columns={\"seqname\": \"chr\"})\n",
    "    df = df[\n",
    "        [\n",
    "            \"dhs_id\",\n",
    "            \"chr\",\n",
    "            \"start\",\n",
    "            \"end\",\n",
    "            \"DHS_width\",\n",
    "            \"summit\",\n",
    "            \"numsamples\",\n",
    "            \"total_signal\",\n",
    "            \"component\",\n",
    "            \"proportion\",\n",
    "            \"sequence\",\n",
    "        ]\n",
    "    ]\n",
    "\n",
    "    # Step 6: Download and process binary peak matrix\n",
    "    print(\"\\n=== Step 6: Downloading and processing binary peak matrix ===\")\n",
    "    binary_gz = data_dir / \"dat_bin_FDR01_hg38.txt.gz\"\n",
    "    binary_file = data_dir / \"dat_bin_FDR01_hg38.txt\"\n",
    "\n",
    "    # Download the file from Google Drive if it doesn't exist\n",
    "    if not binary_gz.exists() or force_download:\n",
    "        print(\"Downloading binary peak matrix from Google Drive...\")\n",
    "        gdown.download(\n",
    "            \"https://drive.google.com/uc?export=download&id=1Nel7wWOWhWn40Yv7eaQFwvpMcQHBNtJ2\",\n",
    "            str(binary_gz),\n",
    "            quiet=False\n",
    "        )\n",
    "\n",
    "    if binary_gz.exists() and not binary_file.exists():\n",
    "        decompress_gz_file(str(binary_gz), force_download)\n",
    "\n",
    "    print(\"Loading binary peak matrix...\")\n",
    "    binary_matrix = pd.read_table(str(binary_file), header=None)\n",
    "\n",
    "    # Create column names\n",
    "    celltype_encodeID = [\n",
    "        row[\"Biosample name\"] + \"_\" + row[\"DCC Library ID\"] for _, row in DHS_metadata.iterrows()\n",
    "    ]\n",
    "    binary_matrix.columns = celltype_encodeID\n",
    "\n",
    "    # Create master dataset\n",
    "    print(\"Creating master dataset...\")\n",
    "    master_dataset = pd.concat([df, binary_matrix], axis=1, sort=False)\n",
    "\n",
    "    # Save as feather file\n",
    "    output_file = data_dir / \"master_dataset.ftr\"\n",
    "    print(f\"Saving master dataset to {output_file}...\")\n",
    "    master_dataset.to_feather(str(output_file))\n",
    "\n",
    "    return master_dataset, DHS_metadata\n",
    "\n",
    "\n",
    "def filter_dataset(\n",
    "    master_dataset_path: Path = Path(\"./data/master_dataset.ftr\"),\n",
    "    cell_list: list = None,\n",
    "    output_path: Path = Path(\"./data/filtered_dataset.txt\"),\n",
    "    sort: bool = True,\n",
    "    balance: bool = True,\n",
    "):\n",
    "    \"\"\"Filter the master dataset for exclusive peaks between replicates.\"\"\"\n",
    "    if cell_list is None:\n",
    "        cell_list = [\"K562_ENCLB843GMH\", \"hESCT0_ENCLB449ZZZ\", \"HepG2_ENCLB029COU\", \"GM12878_ENCLB441ZZZ\"]\n",
    "\n",
    "    print(f\"\\nLoading master dataset from {master_dataset_path}...\")\n",
    "    df = pd.read_feather(str(master_dataset_path))\n",
    "\n",
    "    print(f\"\\nFiltering for cell types: {cell_list}\")\n",
    "    filter_obj = FilteringData(df, cell_list)\n",
    "    filtered_df = filter_obj.filter_exclusive_replicates(sort=sort, balance=balance).reset_index(drop=True)\n",
    "\n",
    "    print(f\"\\nSaving filtered dataset to {output_path}...\")\n",
    "    filtered_df.to_csv((str(output_path)), sep=\"\\t\", index=False)\n",
    "\n",
    "    return filtered_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lEQa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating master dataset...\n",
      "\n",
      "=== Step 1: Downloading and processing genome ===\n",
      "Downloading data/hg38.fa.gz...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded data/hg38.fa.gz\n",
      "Decompressing data/hg38.fa.gz...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decompressed to data/hg38.fa\n",
      "Loading genome...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Step 2: Downloading and processing metadata ===\n",
      "Downloading data/DHS_Index_and_Vocabulary_metadata.tsv...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded data/DHS_Index_and_Vocabulary_metadata.tsv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Step 3: Downloading and processing basis array ===\n",
      "Downloading data/2018-06-08NC16_NNDSVD_Basis.npy.gz...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded data/2018-06-08NC16_NNDSVD_Basis.npy.gz\n",
      "Decompressing data/2018-06-08NC16_NNDSVD_Basis.npy.gz...\n",
      "Decompressed to data/2018-06-08NC16_NNDSVD_Basis.npy\n",
      "\n",
      "=== Step 4: Downloading and processing mixture array (this may take 10+ minutes) ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data/2018-06-08NC16_NNDSVD_Mixture.npy.gz...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded data/2018-06-08NC16_NNDSVD_Mixture.npy.gz\n",
      "Decompressing data/2018-06-08NC16_NNDSVD_Mixture.npy.gz...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decompressed to data/2018-06-08NC16_NNDSVD_Mixture.npy\n",
      "Loading mixture array...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Step 5: Downloading and processing sequence metadata ===\n",
      "Downloading data/DHS_Index_and_Vocabulary_hg38_WM20190703.txt.gz...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded data/DHS_Index_and_Vocabulary_hg38_WM20190703.txt.gz\n",
      "Decompressing data/DHS_Index_and_Vocabulary_hg38_WM20190703.txt.gz...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decompressed to data/DHS_Index_and_Vocabulary_hg38_WM20190703.txt\n",
      "Loading sequence metadata...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l0/6nnd3xcj65d77cy7d4xft29h0000gn/T/marimo_35871/__marimo__cell_bkHC_.py:248: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  sequence_metadata = pd.read_table(str(seq_metadata_file), sep=\"\\t\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding sequences from genome...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Step 6: Downloading and processing binary peak matrix ===\n",
      "Downloading binary peak matrix from Google Drive...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?export=download&id=1Nel7wWOWhWn40Yv7eaQFwvpMcQHBNtJ2\n",
      "From (redirected): https://drive.google.com/uc?export=download&id=1Nel7wWOWhWn40Yv7eaQFwvpMcQHBNtJ2&confirm=t&uuid=0f910fb2-5688-49d8-a40b-87d75362a132\n",
      "To: /Users/simonsenan/Documents/DNA-Diffusion/tutorials/data/dat_bin_FDR01_hg38.txt.gz\n",
      "\r\n",
      "  0%|                                                                                 | 0.00/88.4M [00:00<?, ?B/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "  1%|▍                                                                        | 524k/88.4M [00:00<00:21, 4.11MB/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "  5%|███▍                                                                    | 4.19M/88.4M [00:00<00:04, 20.9MB/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 12%|████████▌                                                               | 10.5M/88.4M [00:00<00:02, 37.1MB/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 20%|██████████████                                                          | 17.3M/88.4M [00:00<00:01, 46.9MB/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 27%|███████████████████▏                                                    | 23.6M/88.4M [00:00<00:01, 51.0MB/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 35%|█████████████████████████▏                                              | 30.9M/88.4M [00:00<00:01, 57.0MB/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 42%|██████████████████████████████▎                                         | 37.2M/88.4M [00:00<00:00, 58.1MB/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 50%|███████████████████████████████████▉                                    | 44.0M/88.4M [00:00<00:00, 61.0MB/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 58%|█████████████████████████████████████████▍                              | 50.9M/88.4M [00:00<00:00, 62.3MB/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 65%|██████████████████████████████████████████████▉                         | 57.7M/88.4M [00:01<00:00, 64.0MB/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 73%|████████████████████████████████████████████████████▌                   | 64.5M/88.4M [00:01<00:00, 64.7MB/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 81%|██████████████████████████████████████████████████████████              | 71.3M/88.4M [00:01<00:00, 56.1MB/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 88%|███████████████████████████████████████████████████████████████▏        | 77.6M/88.4M [00:01<00:00, 55.4MB/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 94%|███████████████████████████████████████████████████████████████████▉    | 83.4M/88.4M [00:01<00:00, 54.3MB/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "100%|████████████████████████████████████████████████████████████████████████| 88.4M/88.4M [00:01<00:00, 53.7MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading binary peak matrix...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(35871,0x209386c80) malloc: Failed to allocate segment from range group - out of space\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating master dataset...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving master dataset to data/master_dataset.ftr...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading master dataset from data/master_dataset.ftr...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Filtering for cell types: ['K562_ENCLB843GMH', 'hESCT0_ENCLB449ZZZ', 'HepG2_ENCLB029COU', 'GM12878_ENCLB441ZZZ']\n",
      "Filtering exclusive peaks between replicates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell type: GM12878, Replicate: ENCLB441ZZZ, Number of exclusive peaks: 11968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell type: HepG2, Replicate: ENCLB029COU, Number of exclusive peaks: 73621\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell type: K562, Replicate: ENCLB843GMH, Number of exclusive peaks: 71106\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell type: hESCT0, Replicate: ENCLB449ZZZ, Number of exclusive peaks: 202853\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving filtered dataset to data/filtered_dataset.txt...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Filtered dataset shape: (47872, 18)\n",
      "Cell type distribution:\n",
      "TAG\n",
      "GM12878_ENCLB441ZZZ    11968\n",
      "HepG2_ENCLB029COU      11968\n",
      "K562_ENCLB843GMH       11968\n",
      "hESCT0_ENCLB449ZZZ     11968\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "data_dir = Path(\"./data\")\n",
    "force_download = True\n",
    "cell_list = [\"K562_ENCLB843GMH\", \"hESCT0_ENCLB449ZZZ\", \"HepG2_ENCLB029COU\", \"GM12878_ENCLB441ZZZ\"]\n",
    "def main():\n",
    "    print(\"Creating master dataset...\")\n",
    "    master_dataset, metadata = create_master_dataset(data_dir, force_download)\n",
    "    if master_dataset is None:\n",
    "        print(\"\\nFailed to create master dataset. Please download binary peak matrix file manually.\")\n",
    "        return\n",
    "\n",
    "    master_dataset_path = data_dir / \"master_dataset.ftr\"\n",
    "    if not master_dataset_path.exists():\n",
    "        print(f\"\\nError: Master dataset not found at {master_dataset_path}\")\n",
    "        print(\"Please run without --filter-only flag first to create the dataset.\")\n",
    "        return\n",
    "\n",
    "    output_path = data_dir / \"filtered_dataset.txt\"\n",
    "    filtered_df = filter_dataset(\n",
    "        master_dataset_path,\n",
    "        cell_list,\n",
    "        output_path,\n",
    "        sort=True,\n",
    "        balance=TrueC\n",
    "    )\n",
    "\n",
    "    print(f\"\\nFiltered dataset shape: {filtered_df.shape}\")\n",
    "    print(f\"Cell type distribution:\")\n",
    "    print(filtered_df[\"TAG\"].value_counts())\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
