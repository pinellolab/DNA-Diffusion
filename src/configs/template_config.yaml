ckpt: null # path to checkpoint
seed: 42

model:
  target: models.diffusion.DDPM
  params:
    beta_end: 0.05
    schedule: linear
    timesteps: 1000
    is_conditional: True
    criterion: utils.metrics.MetricName
    use_ema: True
    lr_warmup: 5000
    lr_scheduler_config:
      target: torch.optim.lr_scheduler.MultiStepLR
      params:
        milestones: [5, 10, 20]
        gamma: 0.1

    optimizer_config:
      target: torch.optim.Adam
      params:
        lr: 1e-4

    unet_config:
      target: models.unets.unet_lucas.UNetLucas
      params:
        dim: 200
        init_dim: None
        dim_mults: [1, 2, 4]
        channels: 1
        resnet_block_groups: 8
        learned_sinusoidal_dim: 16

train_dl:
  _target_: src.data.sequence_dataloader.SequenceDataModule
  train_path: # add training dataset path
  batch_size: 32
  num_workers: 4
  transform:
    _target_: # directly reference transforms -> torchvision.transforms.ToTensor

val_dl:
  _target_: src.data.sequence_dataloader.SequenceDataModule
  val_path: # add validation dataset path
  batch_size: 32
  num_workers: 4
  transform:
    _target_: # directly reference transforms -> torchvision.transforms.ToTensor

lightning:
  callbacks:
    save_checkpoint:
      target: pytorch_lightning.callbacks.ModelCheckpoint
      params:
        path: ""
        monitor: val_loss
        mode: min
        save_top_k: 10
        save_last: True
    learning_rate:
      target: pytorch_lightning.callbacks.LearningRateMonitor
      params:
        logging_interval: epoch

  trainer:
    benchmark: True
    ckpt_dir:
    accelerator: gpu
    strategy: ddp
    min_epochs: 5
    max_epochs: 100000
    gradient_clip_val: 1.0
    accumulate_grad_batches: 1
    log_every_n_steps: 1
    check_val_every_n_epoch: 1 #for debug purposes
    save_last: True
    precision: 32

  hydra:
    run:
      dir:
